{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfedf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size;\n",
    "        self.rotate_list = list(range(-1, batch_size-1))\n",
    "        self.off_diag = torch.eye(batch_size).ge(1).logical_not()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.qfirst_linear = nn.Linear(768, hidden_size)\n",
    "        self.cfirst_linear = nn.Linear(768, hidden_size)\n",
    "        self.cosine = nn.CosineSimilarity(dim=-1)\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        self = super().to(*args, **kwargs) \n",
    "        return self\n",
    "    \n",
    "    def forward(self, query, code):\n",
    "        query = self.qfirst_linear(query)\n",
    "        code = self.cfirst_linear(code)\n",
    "        \n",
    "        if self.training:\n",
    "            q = query.repeat(1, self.batch_size).reshape(self.batch_size,self.batch_size,self.hidden_size)\n",
    "            c = code.repeat(self.batch_size, 1).reshape(self.batch_size,self.batch_size,self.hidden_size)\n",
    "            w = 1\n",
    "            loss = -self.cosine(q,c).diagonal().sum() * w /self.batch_size\n",
    "            loss += torch.masked_select(self.cosine(q,c), self.off_diag.to('cuda')).sum() / (self.batch_size * (self.batch_size - 1))\n",
    "        else:\n",
    "            loss = 0\n",
    "        return {'output_query': query, 'output_code': code,'loss': loss}\n",
    "\n",
    "class Model_deep(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_size, depth):\n",
    "        super(Model_deep, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size;\n",
    "        self.rotate_list = list(range(-1, batch_size-1))\n",
    "        self.off_diag = torch.eye(batch_size).ge(1).logical_not()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.qfirst_linear = nn.Linear(768, hidden_size)\n",
    "        self.cfirst_linear = nn.Linear(768, hidden_size)\n",
    "        self.qlinear_layers = [nn.Linear(hidden_size, hidden_size) for i in range(depth)]\n",
    "        self.clinear_layers = [nn.Linear(hidden_size, hidden_size) for i in range(depth)]\n",
    "        self.cosine = nn.CosineSimilarity(dim=-1)\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        self = super().to(*args, **kwargs) \n",
    "        for lin in self.qlinear_layers:\n",
    "            lin.to(*args, **kwargs)\n",
    "        for lin in self.clinear_layers:\n",
    "            lin.to(*args, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def forward(self, query, code):\n",
    "        query = self.qfirst_linear(query)\n",
    "        for lin in self.qlinear_layers:\n",
    "            query = self.relu(query)\n",
    "            query = lin(query)\n",
    "        code = self.cfirst_linear(code)\n",
    "        for lin in self.clinear_layers:\n",
    "            code = self.relu(code)\n",
    "            code = lin(code)\n",
    "        \n",
    "        if self.training:\n",
    "            q = query.repeat(1, self.batch_size).reshape(self.batch_size,self.batch_size,self.hidden_size)\n",
    "            c = code.repeat(self.batch_size, 1).reshape(self.batch_size,self.batch_size,self.hidden_size)\n",
    "            w = 1\n",
    "            loss = -self.cosine(q,c).diagonal().sum() * w /self.batch_size\n",
    "            loss += torch.masked_select(self.cosine(q,c), self.off_diag.to('cuda')).sum() / (self.batch_size * (self.batch_size - 1))\n",
    "        else:\n",
    "            loss = 0\n",
    "        return {'output_query': query, 'output_code': code,'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING / EVALUATION\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import os\n",
    "\n",
    "encodings_name = 'average_pooling_outputs_v2'\n",
    "\n",
    "def train(batch_size=512, \n",
    "          num_epochs=3, \n",
    "          learning_rate = 5e-5, \n",
    "          warmup_percentage=1, \n",
    "          show_progress=True, \n",
    "          validate=True,\n",
    "          shuffle=True,\n",
    "          hidden=128,\n",
    "          depth=0,\n",
    "          name='model'):\n",
    "    \n",
    "    device = 'cuda'\n",
    "    hidden_size = 768\n",
    "    \n",
    "    filenames = {'train': './data/train/{}'.format(encodings_name), \n",
    "                 'validation': './data/validation/{}'.format(encodings_name), \n",
    "                 'test': './data/test/{}'.format(encodings_name)}\n",
    "    num_samples = {'train': 412178, 'validation': 23107, 'test': 22176}\n",
    "\n",
    "    samples = torch.FloatTensor(torch.FloatStorage.from_file(filenames['train'], shared=False, size=num_samples['train'] * 2 * hidden_size)).reshape(num_samples['train'], 2, hidden_size)\n",
    "    train_sampler = torch.utils.data.RandomSampler(samples)\n",
    "    train_loader = DataLoader(samples, sampler=train_sampler, batch_size=batch_size, num_workers=0, drop_last=True)\n",
    "\n",
    "    model = Model(batch_size, hidden) #If testing deeper model, use Model_deep\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    num_warmup_steps = round(num_epochs * len(train_loader) * warmup_percentage / (batch_size * 100)) * batch_size\n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\"linear\", \n",
    "                                 optimizer = optimizer, \n",
    "                                 num_warmup_steps = num_warmup_steps, \n",
    "                                 num_training_steps = num_training_steps)\n",
    "    losses = []\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "    best_mrr = 0\n",
    "    best_epoch = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for i, batch in enumerate(tqdm(train_loader, disable=not show_progress)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                query = batch[:,0,:].to(device)\n",
    "                code = batch[:,1,:].to(device)\n",
    "\n",
    "            output = model(query, code)\n",
    "\n",
    "            loss = output['loss']\n",
    "            losses += [loss.detach().cpu()]\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % round(len(train_loader)/10) == 1:\n",
    "                print(f'Epoch ({round(i/len(train_loader), 1)*100 - 10} - {round(i/len(train_loader), 1)*100} %): {epoch+1} \\t Average loss: {sum(losses[-round(len(train_loader)/10):])/len(losses[-math.floor(len(train_loader)/10):])}')\n",
    "        print(f'Epoch: {epoch+1} \\t Average loss: {sum(losses[-round(len(train_loader)):])/len(losses[-math.floor(len(train_loader)):])}')\n",
    "\n",
    "        mrr = test(model=model, hidden=hidden, split='validation', show_progress=False, name=name)\n",
    "        if mrr > best_mrr:\n",
    "            best_mrr = mrr\n",
    "            best_epoch = epoch+1\n",
    "            torch.save(model.state_dict(), './{}/MRR({})_epoch({})'.format(name, best_mrr, epoch+1))\n",
    "        else:\n",
    "            print(f\"Don't save epoch {epoch+1}, it had a worse MRR\")\n",
    "            break\n",
    "        \n",
    "    plt.plot(losses)\n",
    "    return best_mrr, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192d8cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df99e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING / EVALUATION\n",
    "import math\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def test(model=None, split='test', show_progress=True, eval_size=512, hidden=128, depth=1, name='model'):\n",
    "    if model is None:\n",
    "        model = Model(eval_size, hidden)\n",
    "        model.load_state_dict(torch.load('./{}'.format(name)))\n",
    "    model.eval()\n",
    "    \n",
    "    MRRs = []\n",
    "    hidden_size = 768\n",
    "    device = 'cuda'\n",
    "\n",
    "    filenames = {'train': './data/train/{}'.format(encodings_name), \n",
    "                 'validation': './data/validation/{}'.format(encodings_name), \n",
    "                 'test': './data/test/{}'.format(encodings_name)}\n",
    "    num_samples = {'train': 412178, 'validation': 23107, 'test': 22176}\n",
    "\n",
    "    samples = torch.FloatTensor(torch.FloatStorage.from_file(filenames[split], shared=False, size=num_samples[split] * 2 * hidden_size)).reshape(num_samples[split], 2, hidden_size)\n",
    "    loader = DataLoader(samples, batch_size=eval_size, num_workers=0, drop_last=True)\n",
    "\n",
    "    cosine = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(device)\n",
    "\n",
    "        for batch in tqdm(loader, disable=not show_progress):\n",
    "            cos = torch.tensor([]).to(device)\n",
    "            max_size = 512\n",
    "            for mini_batch in [batch[:512], batch[:512]]:\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    query = mini_batch[:,0,:].to(device)\n",
    "                    code = mini_batch[:,1,:].to(device)\n",
    "\n",
    "                output = model(query, code)\n",
    "\n",
    "                q = output['output_query'].repeat(1, max_size).reshape(max_size,max_size,hidden)\n",
    "                c = output['output_code'].repeat(max_size, 1).reshape(max_size,max_size,hidden)\n",
    "\n",
    "                cos = torch.cat((cos, cosine(q, c)))\n",
    "            MRRs += (1/(cos.argsort(descending=True).argsort(descending=False).diagonal() + 1)).tolist()\n",
    "            \n",
    "            mrr = round(sum(MRRs)/len(MRRs), 4)\n",
    "        plt.hist([1/m for m in MRRs], bins=128)\n",
    "        print('MRR: {}, Rank: {}'.format(mrr, 1/mrr))\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82979120",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size=512\n",
    "learning_rate = 5e-5\n",
    "shuffle=True\n",
    "warmup_percentage=0.0\n",
    "num_epochs=20\n",
    "depths=1\n",
    "depth=0\n",
    "\n",
    "dir_name = 'BERT_VS_CodeBERTa'\n",
    "if not os.path.exists(f'./{dir_name}'): \n",
    "        os.mkdir(f'./{dir_name}')\n",
    "\n",
    "for hidden in [512]:\n",
    "    for learning_rate in [5e-6]:\n",
    "        for batch_size in [512]:\n",
    "            for depth in [1,2,3]:\n",
    "\n",
    "                name = '{}/depth({})_hidden({})_shuffle({})_batchsize({})_lnrate({})_wrmup({})'.format(dir_name,\n",
    "                                                                                                       depth, \n",
    "                                                                                                       hidden, \n",
    "                                                                                                       shuffle, \n",
    "                                                                                                       batch_size, \n",
    "                                                                                                       learning_rate, \n",
    "                                                                                                       warmup_percentage)\n",
    "                if not os.path.exists(f'./{name}'): \n",
    "                    os.mkdir(f'./{name}')\n",
    "\n",
    "                print('{}\\nTRAINING: {}\\n{}'.format('-'*100,name,'-'*100))\n",
    "                best_mrr, epoch = train(batch_size=batch_size, \n",
    "                                        hidden=hidden, \n",
    "                                        depth = depth, \n",
    "                                        num_epochs=num_epochs, \n",
    "                                        learning_rate = learning_rate, \n",
    "                                        shuffle=shuffle, \n",
    "                                        warmup_percentage=warmup_percentage,\n",
    "                                        name=name,\n",
    "                                        show_progress=True)\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                name = './{}/MRR({})_epoch({})'.format(name, best_mrr, epoch)\n",
    "                test(show_progress=True, eval_size=512, hidden=hidden, depth=depth, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "dir_name = 'BERT_VS_CodeBERTa'\n",
    "\n",
    "depth=0\n",
    "hidden=512\n",
    "batch_size=512\n",
    "learning_rate = 5e-6\n",
    "shuffle=True\n",
    "warmup_percentage=0.0\n",
    "\n",
    "mrr = 0.1121\n",
    "epoch=7\n",
    "\n",
    "model_config = '{}/depth({})_hidden({})_shuffle({})_batchsize({})_lnrate({})_wrmup({})'.format(dir_name,\n",
    "                                                                                               depth, \n",
    "                                                                                               hidden, \n",
    "                                                                                               shuffle, \n",
    "                                                                                               batch_size, \n",
    "                                                                                               learning_rate, \n",
    "                                                                                               warmup_percentage)\n",
    "\n",
    "\n",
    "name = '{}/MRR({})_epoch({})'.format(model_config, mrr, epoch)\n",
    "mrr = test(show_progress=True, eval_size=1000, hidden=512, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e61906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
